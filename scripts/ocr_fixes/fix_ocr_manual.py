import json
import re
import os

# === Fun√ß√µes utilit√°rias para limpeza de duplica√ß√£o de t√≠tulo de cap√≠tulo ===
def _normalize_label(text: str) -> str:
    """Normaliza texto para compara√ß√£o tolerante (min√∫sculas, sem pontua√ß√£o extra)."""
    if text is None:
        return ""
    ZW_CHARS = [
        '\u200b', '\u200c', '\u200d', '\ufeff',  # zero-width + BOM
    ]
    NBSP_CHARS = [
        '\u00a0', '\u202f', '\u2009', '\u200a'  # non-breaking / narrow / thin spaces
    ]
    t = text
    for ch in ZW_CHARS:
        t = t.replace(ch, '')
    for ch in NBSP_CHARS:
        t = t.replace(ch, ' ')
    t = t.replace('‚Äî', ' ').replace('‚Äì', ' ').replace('-', ' ')
    t = t.lower()
    import re as _re
    t = _re.sub(r"[^\w\s]", " ", t, flags=_re.UNICODE)
    t = _re.sub(r"\s+", " ", t).strip()
    return t

import re as _re
def clean_repeated_chapter_title(structure):
    removed = 0
    trimmed = 0
    title_re = _re.compile(r"^\s*CHAPTER\s+([IVXLCDM]+)\.\s*(.+)$", _re.IGNORECASE)
    for section in structure:
        for ch in section.get('chapters', []):
            title = ch.get('chapter_title', '') or ''
            m = title_re.match(title)
            if not m:
                continue
            label = m.group(2).strip()
            if not label:
                continue
            norm_label = _normalize_label(label)
            content_list = ch.get('content', []) or []
            p_index = None
            for idx, item in enumerate(content_list):
                if isinstance(item, dict) and item.get('type') == 'p' and item.get('content'):
                    p_index = idx
                    break
            if p_index is None:
                continue
            para = content_list[p_index]
            para_text = para.get('content', '') or ''
            if not para_text.strip():
                continue
            norm_para = _normalize_label(para_text)
            if norm_para == norm_label:
                content_list.pop(p_index)
                removed += 1
                continue
            prefix_re = _re.compile(r"^\s*" + _re.escape(label) + r"[\s\.:;,_\-‚Äî]*", flags=_re.IGNORECASE)
            m2 = prefix_re.match(para_text)
            if m2:
                rest = para_text[m2.end():].lstrip(" .,:;-‚Äî_")
                if not rest.strip():
                    content_list.pop(p_index)
                    removed += 1
                else:
                    para['content'] = rest
                    if 'word_count' in para:
                        para['word_count'] = len(rest.split())
                    trimmed += 1
                continue
            if norm_para.startswith(norm_label + " ") or norm_para.startswith(norm_label + ":"):
                start_ci = para_text[:len(label)]
                if start_ci.lower() == label.lower():
                    rest = para_text[len(label):].lstrip(" .,:;-‚Äî_")
                else:
                    label_words = label.split()
                    rest_words = para_text.split()
                    k = len(label_words)
                    rest = " ".join(rest_words[k:])
                if not rest.strip():
                    content_list.pop(p_index)
                    removed += 1
                else:
                    para['content'] = rest
                    if 'word_count' in para:
                        para['word_count'] = len(rest.split())
                    trimmed += 1
    return removed, trimmed

def is_paragraph_continuation(text1, text2):
    """
    Detecta se o segundo par√°grafo √© uma continua√ß√£o do primeiro.
    Crit√©rios:
    1. Primeiro par√°grafo termina sem pontua√ß√£o final (. ! ? :)
    2. Segundo par√°grafo n√£o come√ßa com mai√∫scula ou palavra de in√≠cio de par√°grafo
    3. Primeiro par√°grafo n√£o √© muito curto (evitar t√≠tulos)
    4. Contexto sem√¢ntico indica continua√ß√£o
    """
    if not text1 or not text2:
        return False
    
    text1 = text1.strip()
    text2 = text2.strip()
    
    # Verifica se o primeiro par√°grafo √© muito curto (prov√°vel t√≠tulo)
    # Mas permite exce√ß√µes se h√° indicadores claros de continua√ß√£o
    words1 = text1.split()
    if len(words1) < 4:  # Muito curto mesmo
        return False
    elif len(words1) < 8:  # Curto, mas verifica indicadores de continua√ß√£o
        # Se come√ßa com min√∫scula ou termina com palavra mid-sentence, pode ser continua√ß√£o
        if text2 and text2[0].islower():
            pass  # Continua verifica√ß√£o
        elif words1 and words1[-1].lower() in ['though', 'although', 'while', 'when', 'where', 'which', 'that', 'who', 'whom', 'whose', 'if', 'unless', 'until', 'since', 'because', 'as', 'before', 'after']:
            pass  # Continua verifica√ß√£o
        else:
            return False  # Muito curto sem indicadores de continua√ß√£o
    
    # Verifica se o primeiro par√°grafo termina sem pontua√ß√£o final
    if text1[-1] in '.!?:':
        return False
    
    # Verifica se o segundo par√°grafo come√ßa com min√∫scula (indica√ß√£o de continua√ß√£o)
    if text2[0].islower():
        return True
    
    # Palavras que indicam continua√ß√£o quando em mai√∫scula
    continuation_words = {
        'And', 'But', 'For', 'Or', 'Nor', 'So', 'Yet', 'However', 'Therefore',
        'Thus', 'Hence', 'Moreover', 'Furthermore', 'Nevertheless', 'Nonetheless',
        'Another', 'Others', 'These', 'Those', 'Such', 'Many', 'Some', 'All',
        'Both', 'Either', 'Neither', 'Each', 'Every', 'Any', 'No', 'None'
    }
    
    first_word = text2.split()[0] if text2.split() else ""
    
    # Se come√ßa com palavra de continua√ß√£o, provavelmente √© continua√ß√£o
    if first_word in continuation_words:
        return True
    
    # Verifica se termina com v√≠rgula, ponto e v√≠rgula, ou outros indicadores de continua√ß√£o
    if text1[-1] in ',;-':
        return True
    
    # Verifica padr√µes espec√≠ficos de quebra mid-sentence
    # Como no exemplo: "though he" seguido de "immediately afterwards"
    mid_sentence_endings = ['though', 'although', 'while', 'when', 'where', 'which', 'that', 'who', 'whom', 'whose', 'if', 'unless', 'until', 'since', 'because', 'as', 'before', 'after']
    last_word = text1.split()[-1].lower() if text1.split() else ""
    
    if last_word in mid_sentence_endings:
        return True
    
    return False

def merge_broken_paragraphs(chapters):
    """
    Mescla par√°grafos que foram quebrados incorretamente pelo OCR.
    Retorna n√∫mero de mesclagens realizadas.
    """
    merges_count = 0
    
    for chapter in chapters:
        if 'content' not in chapter:
            continue
            
        content = chapter['content']
        if len(content) < 2:
            continue
        
        # Processar de tr√°s para frente para n√£o afetar √≠ndices
        i = len(content) - 1
        while i > 0:
            # Ajustar i caso a lista tenha encolhido (por remo√ß√£o anterior)
            if i >= len(content):
                i = len(content) - 1
                if i <= 0:
                    break
            if i - 1 < 0:
                break
                
            current = content[i]
            previous = content[i-1]
            
            # Apenas processar par√°grafos de texto
            if (current.get('type') == 'p' and previous.get('type') == 'p' and 
                'content' in current and 'content' in previous):
                
                prev_text = previous['content']
                curr_text = current['content']
                
                if is_paragraph_continuation(prev_text, curr_text):
                    # Mesclar os par√°grafos
                    merged_text = prev_text + " " + curr_text
                    previous['content'] = merged_text
                    previous['word_count'] = len(merged_text.split())
                    
                    # Remover o par√°grafo atual
                    content.pop(i)
                    merges_count += 1
                    
                    # Continue verificando a mesma posi√ß√£o i (agora apontando para o pr√≥ximo elemento)
                    # Se removemos o √∫ltimo elemento, o ajuste de i no topo do la√ßo cuidar√° do limite
                    continue
            
            i -= 1
    
    return merges_count

def fix_ocr_manual_only(text):
    """
    Corrige APENAS problemas de OCR atrav√©s de corre√ß√µes manuais espec√≠ficas.
    N√ÉO aplica nenhuma regex autom√°tica que possa quebrar palavras v√°lidas.
    """
    # APENAS corre√ß√µes manuais espec√≠ficas e verificadas
    manual_fixes = {
        # Problemas reais de concatena√ß√£o religiosa
        'beforeGod': 'before God',
        'toGod': 'to God', 
        'ofGod': 'of God',
        'withGod': 'with God',
        'fromGod': 'from God',
        'forGod': 'for God',
        'inGod': 'in God',
        'ourLord': 'our Lord',
        'ourSaviour': 'our Saviour',
        'ourSavior': 'our Savior',
        'JesusChrist': 'Jesus Christ',
        'HolyGhost': 'Holy Ghost',
        'HolySpirit': 'Holy Spirit',
        'BlessedVirgin': 'Blessed Virgin',
        'DivineMajesty': 'Divine Majesty',
        'DivineGoodness': 'Divine Goodness',
        
        # Problemas de vida espiritual
        'eternallife': 'eternal life',
        'spirituallife': 'spiritual life', 
        'devoutlife': 'devout life',
        
        # Problemas comuns de OCR
        'morethan': 'more than',
        'lessthan': 'less than',
        'ratherthan': 'rather than',
        'otherthan': 'other than',
        'everday': 'every day',
        'somethimes': 'sometimes',
        'sometmes': 'sometimes',
        
        # Igreja e textos sagrados
        'theChurch': 'the Church',
        'theGospel': 'the Gospel',
        'theBible': 'the Bible',
        'theScripture': 'the Scripture',
        'theScriptures': 'the Scriptures',
        'theSacrament': 'the Sacrament',
        'theSacraments': 'the Sacraments',
        
        # Preposi√ß√µes grudadas
        'prayerto': 'prayer to',
        'devotedto': 'devoted to',
        'unitedto': 'united to',
        'attachedto': 'attached to',
        'subjectedto': 'subjected to',
        'dedicatedto': 'dedicated to',
        
        # Palavras latinas
        'PaterNoster': 'Pater Noster',
        'AveMaria': 'Ave Maria',
        'TeDeumLaudamus': 'Te Deum Laudamus',
        'VeniCreator': 'Veni Creator',
        
        # Problemas de pontua√ß√£o
        '.‚Äî': '. ‚Äî',
        ',‚Äî': ', ‚Äî',
        ';‚Äî': '; ‚Äî',
        ':‚Äî': ': ‚Äî',
        
        # Cap√≠tulos - APENAS quando s√£o realmente grudados
        'ChapterI': 'Chapter I',
        'ChapterII': 'Chapter II', 
        'ChapterIII': 'Chapter III',
        'ChapterIV': 'Chapter IV',
        'ChapterV': 'Chapter V',
        'ChapterVI': 'Chapter VI',
        'ChapterVII': 'Chapter VII',
        'ChapterVIII': 'Chapter VIII',
        'ChapterIX': 'Chapter IX',
        'ChapterX': 'Chapter X',
        
        # Problemas espec√≠ficos encontrados no texto
        'andCredoin': 'and Credo in',
        'MeeknesstowardsOurselves': 'Meekness towards Ourselves',
        'plantsoftheChurch': 'plants of the Church',
        'loveofthe': 'love of the',
        'loveof': 'love of',
        'fearof': 'fear of',
        'desireof': 'desire of',
        'hopeof': 'hope of',
        'faithin': 'faith in',
        'trustin': 'trust in',
        'believein': 'believe in',
        'confidencein': 'confidence in',
    }
    
    # Aplicar APENAS as corre√ß√µes manuais
    fixed_text = text
    changes = 0
    
    for wrong, correct in manual_fixes.items():
        if wrong in fixed_text:
            # Usar replace simples, n√£o regex
            before = fixed_text
            fixed_text = fixed_text.replace(wrong, correct)
            if before != fixed_text:
                changes += 1
    
    # NENHUMA regex autom√°tica! Apenas limpar espa√ßos duplos
    fixed_text = re.sub(r'  +', ' ', fixed_text)
    
    return fixed_text.strip(), changes

def fix_json_manual_only(input_file, output_file=None):
    """
    Aplica corre√ß√µes APENAS manuais ao JSON e mescla par√°grafos quebrados
    """
    if output_file is None:
        output_file = input_file
    
    # Backup
    if output_file == input_file:
        backup_file = input_file.replace('.json', '_backup_manual.json')
        if os.path.exists(input_file):
            import shutil
            shutil.copy2(input_file, backup_file)
            print(f"üíæ Backup: {backup_file}")
    
    # Carregar JSON
    with open(input_file, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    # ETAPA 1: Mesclar par√°grafos quebrados ANTES das corre√ß√µes de OCR
    print("üîó Mesclando par√°grafos quebrados...")
    all_chapters = []
    for part in data:
        all_chapters.extend(part.get('chapters', []))
    merges_count = merge_broken_paragraphs(all_chapters)
    if merges_count > 0:
        print(f"   ‚úÖ {merges_count} par√°grafos mesclados")
    else:
        print("   ‚ÑπÔ∏è Nenhum par√°grafo quebrado detectado")

    # ETAPA 1.5: Remover duplica√ß√£o de t√≠tulo de cap√≠tulo ap√≥s mesclagem
    removed, trimmed = clean_repeated_chapter_title(data)
    print(f"üßº clean_repeated_chapter_title: Removidos: {removed}, Ajustados: {trimmed}")
    
    # ETAPA 2: Corre√ß√µes manuais de OCR
    print("üîß Aplicando corre√ß√µes manuais de OCR...")
    total_corrections = 0
    total_items = 0
    examples_shown = 0
    
    # Processar dados
    for part in data:
        # T√≠tulos de partes
        if 'part_title' in part:
            total_items += 1
            original = part['part_title']
            corrected, changes = fix_ocr_manual_only(original)
            if changes > 0:
                part['part_title'] = corrected
                total_corrections += changes
                if examples_shown < 3:
                    print(f"‚úèÔ∏è  Parte: '{original}' ‚Üí '{corrected}'")
                    examples_shown += 1
        
        # Cap√≠tulos
        for chapter in part.get('chapters', []):
            if 'chapter_title' in chapter:
                total_items += 1
                original = chapter['chapter_title']
                corrected, changes = fix_ocr_manual_only(original)
                if changes > 0:
                    chapter['chapter_title'] = corrected
                    total_corrections += changes
                    if examples_shown < 5:
                        print(f"‚úèÔ∏è  Cap: '{original}' ‚Üí '{corrected}'")
                        examples_shown += 1
            
            # Conte√∫do
            for paragraph in chapter.get('content', []):
                if 'content' in paragraph:
                    total_items += 1
                    original = paragraph['content']
                    corrected, changes = fix_ocr_manual_only(original)
                    if changes > 0:
                        paragraph['content'] = corrected
                        paragraph['word_count'] = len(corrected.split())
                        total_corrections += changes
                        if examples_shown < 8:
                            print(f"‚úèÔ∏è  Texto: '{original[:50]}...' ‚Üí '{corrected[:50]}...'")
                            examples_shown += 1
    
    # Salvar
    # Recompute word_count for every content item as the last step before saving
    def _recompute_counts(struct):
        items = 0
        for part in struct:
            for ch in part.get('chapters', []):
                for it in ch.get('content', []):
                    if isinstance(it, dict) and 'content' in it:
                        it['word_count'] = len((it.get('content') or '').split())
                        items += 1
        return items
    recomputed_items = _recompute_counts(data)
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(data, f, indent=2, ensure_ascii=False)
    
    print(f"\nüìä RESULTADO MANUAL:")
    print(f"   Par√°grafos mesclados: {merges_count}")
    print(f"   Itens processados: {total_items}")
    print(f"   Corre√ß√µes OCR aplicadas: {total_corrections}")
    print(f"   üî¢ word_count recalculado em {recomputed_items} itens")
    print(f"   Arquivo: {output_file}")
    
    return total_corrections + merges_count

def main():
    """Fun√ß√£o principal - Corre√ß√µes manuais e mesclagem de par√°grafos"""
    print("‚úã CORRETOR MANUAL DE OCR")
    print("Aplica SOMENTE corre√ß√µes manuais espec√≠ficas")
    print("Mescla par√°grafos quebrados incorretamente pelo OCR")
    print("N√£o quebra palavras v√°lidas como 'Description' ou 'Devotion'")
    print("=" * 70)
    
    # Processar apenas o arquivo em ingl√™s (fonte original com problemas de OCR)
    # Detectar diret√≥rio base do projeto
    script_dir = os.path.dirname(os.path.abspath(__file__))
    project_root = os.path.dirname(os.path.dirname(script_dir))
    file_path = os.path.join(project_root, 'output', 'livro_en.json')
    
    if os.path.exists(file_path):
        print(f"\nüìñ Arquivo: {os.path.basename(file_path)}")
        print("‚ÑπÔ∏è  Nota: O arquivo PT-BR √© gerado pelo Google Translate e n√£o precisa de corre√ß√£o OCR")
        
        corrections = fix_json_manual_only(file_path)
        
        if corrections > 0:
            print(f"‚úÖ {corrections} corre√ß√µes totais aplicadas")
        else:
            print("‚úÖ Nenhuma corre√ß√£o necess√°ria")
    else:
        print(f"‚ùå N√£o encontrado: {file_path}")
    
    print(f"\n‚úã Corre√ß√£o manual conclu√≠da!")
    print("Par√°grafos quebrados foram mesclados automaticamente.")
    print("Palavras v√°lidas como 'Description' e 'Devotion' foram preservadas.")

if __name__ == "__main__":
    main()

